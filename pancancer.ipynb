{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "# from sklearn.linear_model import LassoCV\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.feature_selection import SelectFromModel, mutual_info_classif\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import SelectFromModel, mutual_info_classif\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_23116\\2128522218.py:2: DtypeWarning: Columns (8960) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv('merged_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>100130426</th>\n",
       "      <th>100133144</th>\n",
       "      <th>100134869</th>\n",
       "      <th>10357</th>\n",
       "      <th>10431</th>\n",
       "      <th>136542</th>\n",
       "      <th>155060</th>\n",
       "      <th>26823</th>\n",
       "      <th>280660</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>Subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-OR-A5J1-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>7.23</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.10</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.44</td>\n",
       "      <td>8.46</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.34</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.18</td>\n",
       "      <td>9.22</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-OR-A5J2-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>6.36</td>\n",
       "      <td>10.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.78</td>\n",
       "      <td>5.86</td>\n",
       "      <td>8.13</td>\n",
       "      <td>11.54</td>\n",
       "      <td>5.02</td>\n",
       "      <td>10.19</td>\n",
       "      <td>11.58</td>\n",
       "      <td>10.89</td>\n",
       "      <td>9.65</td>\n",
       "      <td>Inflammatory (Immune C3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-OR-A5J3-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.92</td>\n",
       "      <td>6.45</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.58</td>\n",
       "      <td>5.35</td>\n",
       "      <td>8.96</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>9.66</td>\n",
       "      <td>11.38</td>\n",
       "      <td>10.53</td>\n",
       "      <td>8.78</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-OR-A5J5-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.78</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.72</td>\n",
       "      <td>4.23</td>\n",
       "      <td>7.69</td>\n",
       "      <td>9.80</td>\n",
       "      <td>3.66</td>\n",
       "      <td>9.12</td>\n",
       "      <td>11.21</td>\n",
       "      <td>10.16</td>\n",
       "      <td>9.01</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-OR-A5J6-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.09</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.89</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.64</td>\n",
       "      <td>9.47</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.90</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Patient ID  100130426  100133144  100134869  10357  10431  136542  \\\n",
       "0  TCGA-OR-A5J1-01        0.0       2.09       2.30   7.23  10.99     0.0   \n",
       "1  TCGA-OR-A5J2-01        0.0       1.88       3.32   6.36  10.35     0.0   \n",
       "2  TCGA-OR-A5J3-01        0.0       1.45       2.92   6.45  10.04     0.0   \n",
       "3  TCGA-OR-A5J5-01        0.0       0.00       1.35   5.78  11.20     0.0   \n",
       "4  TCGA-OR-A5J6-01        0.0       0.00       2.45   6.09  10.30     0.0   \n",
       "\n",
       "   155060  26823  280660  ...  ZWINT  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B  \\\n",
       "0    8.10   1.29     0.0  ...   7.21  4.44  8.46  10.04    0.57    9.34   \n",
       "1    7.65   0.00     0.0  ...   8.78  5.86  8.13  11.54    5.02   10.19   \n",
       "2    8.45   0.67     0.0  ...   7.58  5.35  8.96   9.84    0.67    9.66   \n",
       "3    8.78   0.83     0.0  ...   9.72  4.23  7.69   9.80    3.66    9.12   \n",
       "4    7.23   0.00     0.0  ...   6.00  3.79  6.89   9.81    3.14    9.64   \n",
       "\n",
       "     ZYX  ZZEF1  ZZZ3                          Subtype  \n",
       "0  10.85  10.18  9.22  Lymphocyte Depleted (Immune C4)  \n",
       "1  11.58  10.89  9.65         Inflammatory (Immune C3)  \n",
       "2  11.38  10.53  8.78  Lymphocyte Depleted (Immune C4)  \n",
       "3  11.21  10.16  9.01  Lymphocyte Depleted (Immune C4)  \n",
       "4   9.47   9.64  8.90  Lymphocyte Depleted (Immune C4)  \n",
       "\n",
       "[5 rows x 20534 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load the data\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100133144</th>\n",
       "      <th>100134869</th>\n",
       "      <th>10357</th>\n",
       "      <th>10431</th>\n",
       "      <th>155060</th>\n",
       "      <th>388795</th>\n",
       "      <th>390284</th>\n",
       "      <th>57714</th>\n",
       "      <th>645851</th>\n",
       "      <th>653553</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>Subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>7.23</td>\n",
       "      <td>10.99</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.65</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1.29</td>\n",
       "      <td>10.49</td>\n",
       "      <td>...</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.44</td>\n",
       "      <td>8.46</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.34</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.18</td>\n",
       "      <td>9.22</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>6.36</td>\n",
       "      <td>10.35</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.64</td>\n",
       "      <td>9.05</td>\n",
       "      <td>1.77</td>\n",
       "      <td>7.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.78</td>\n",
       "      <td>5.86</td>\n",
       "      <td>8.13</td>\n",
       "      <td>11.54</td>\n",
       "      <td>5.02</td>\n",
       "      <td>10.19</td>\n",
       "      <td>11.58</td>\n",
       "      <td>10.89</td>\n",
       "      <td>9.65</td>\n",
       "      <td>Inflammatory (Immune C3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.45</td>\n",
       "      <td>2.92</td>\n",
       "      <td>6.45</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>3.12</td>\n",
       "      <td>7.35</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.59</td>\n",
       "      <td>...</td>\n",
       "      <td>7.58</td>\n",
       "      <td>5.35</td>\n",
       "      <td>8.96</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>9.66</td>\n",
       "      <td>11.38</td>\n",
       "      <td>10.53</td>\n",
       "      <td>8.78</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.78</td>\n",
       "      <td>11.20</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.85</td>\n",
       "      <td>5.75</td>\n",
       "      <td>2.04</td>\n",
       "      <td>10.25</td>\n",
       "      <td>...</td>\n",
       "      <td>9.72</td>\n",
       "      <td>4.23</td>\n",
       "      <td>7.69</td>\n",
       "      <td>9.80</td>\n",
       "      <td>3.66</td>\n",
       "      <td>9.12</td>\n",
       "      <td>11.21</td>\n",
       "      <td>10.16</td>\n",
       "      <td>9.01</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.09</td>\n",
       "      <td>10.30</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.17</td>\n",
       "      <td>1.42</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.89</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.64</td>\n",
       "      <td>9.47</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.90</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100133144  100134869  10357  10431  155060  388795  390284  57714  645851  \\\n",
       "0       2.09       2.30   7.23  10.99    8.10    0.00    3.65   8.15    1.29   \n",
       "1       1.88       3.32   6.36  10.35    7.65    0.49    2.64   9.05    1.77   \n",
       "2       1.45       2.92   6.45  10.04    8.45    0.67    3.12   7.35    3.47   \n",
       "3       0.00       1.35   5.78  11.20    8.78    0.83    2.85   5.75    2.04   \n",
       "4       0.00       2.45   6.09  10.30    7.23    1.92    3.54   7.17    1.42   \n",
       "\n",
       "   653553  ...  ZWINT  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B    ZYX  ZZEF1  ZZZ3  \\\n",
       "0   10.49  ...   7.21  4.44  8.46  10.04    0.57    9.34  10.85  10.18  9.22   \n",
       "1    7.62  ...   8.78  5.86  8.13  11.54    5.02   10.19  11.58  10.89  9.65   \n",
       "2    9.59  ...   7.58  5.35  8.96   9.84    0.67    9.66  11.38  10.53  8.78   \n",
       "3   10.25  ...   9.72  4.23  7.69   9.80    3.66    9.12  11.21  10.16  9.01   \n",
       "4    8.70  ...   6.00  3.79  6.89   9.81    3.14    9.64   9.47   9.64  8.90   \n",
       "\n",
       "                           Subtype  \n",
       "0  Lymphocyte Depleted (Immune C4)  \n",
       "1         Inflammatory (Immune C3)  \n",
       "2  Lymphocyte Depleted (Immune C4)  \n",
       "3  Lymphocyte Depleted (Immune C4)  \n",
       "4  Lymphocyte Depleted (Immune C4)  \n",
       "\n",
       "[5 rows x 16336 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete all columns (genes) containing missing values\n",
    "df = merged_data \n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values\n",
    "missing_rows = df.isnull().any(axis=1)\n",
    "df = df.drop(columns=missing_values.index)\n",
    "\n",
    "if 'Patient ID' in df.columns:\n",
    "    df = df.drop(columns=['Patient ID'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9126, 20534)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding 'Subtype' column\n",
    "encoder = LabelEncoder()\n",
    "df['Subtype_encoded'] = encoder.fit_transform(df['Subtype']) + 1\n",
    "\n",
    "# Assuming df has the 'Subtype_encoded' and feature columns\n",
    "X = df.drop(['Subtype', 'Subtype_encoded'], axis=1)\n",
    "y = df['Subtype_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100133144</th>\n",
       "      <th>100134869</th>\n",
       "      <th>10357</th>\n",
       "      <th>10431</th>\n",
       "      <th>155060</th>\n",
       "      <th>388795</th>\n",
       "      <th>390284</th>\n",
       "      <th>57714</th>\n",
       "      <th>645851</th>\n",
       "      <th>653553</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>Subtype</th>\n",
       "      <th>Subtype_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>7.23</td>\n",
       "      <td>10.99</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.65</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1.29</td>\n",
       "      <td>10.49</td>\n",
       "      <td>...</td>\n",
       "      <td>4.44</td>\n",
       "      <td>8.46</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.34</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.18</td>\n",
       "      <td>9.22</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>6.36</td>\n",
       "      <td>10.35</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.64</td>\n",
       "      <td>9.05</td>\n",
       "      <td>1.77</td>\n",
       "      <td>7.62</td>\n",
       "      <td>...</td>\n",
       "      <td>5.86</td>\n",
       "      <td>8.13</td>\n",
       "      <td>11.54</td>\n",
       "      <td>5.02</td>\n",
       "      <td>10.19</td>\n",
       "      <td>11.58</td>\n",
       "      <td>10.89</td>\n",
       "      <td>9.65</td>\n",
       "      <td>Inflammatory (Immune C3)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.45</td>\n",
       "      <td>2.92</td>\n",
       "      <td>6.45</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>3.12</td>\n",
       "      <td>7.35</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.59</td>\n",
       "      <td>...</td>\n",
       "      <td>5.35</td>\n",
       "      <td>8.96</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>9.66</td>\n",
       "      <td>11.38</td>\n",
       "      <td>10.53</td>\n",
       "      <td>8.78</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.78</td>\n",
       "      <td>11.20</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.85</td>\n",
       "      <td>5.75</td>\n",
       "      <td>2.04</td>\n",
       "      <td>10.25</td>\n",
       "      <td>...</td>\n",
       "      <td>4.23</td>\n",
       "      <td>7.69</td>\n",
       "      <td>9.80</td>\n",
       "      <td>3.66</td>\n",
       "      <td>9.12</td>\n",
       "      <td>11.21</td>\n",
       "      <td>10.16</td>\n",
       "      <td>9.01</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.09</td>\n",
       "      <td>10.30</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.17</td>\n",
       "      <td>1.42</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.89</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.64</td>\n",
       "      <td>9.47</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.90</td>\n",
       "      <td>Lymphocyte Depleted (Immune C4)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100133144  100134869  10357  10431  155060  388795  390284  57714  645851  \\\n",
       "0       2.09       2.30   7.23  10.99    8.10    0.00    3.65   8.15    1.29   \n",
       "1       1.88       3.32   6.36  10.35    7.65    0.49    2.64   9.05    1.77   \n",
       "2       1.45       2.92   6.45  10.04    8.45    0.67    3.12   7.35    3.47   \n",
       "3       0.00       1.35   5.78  11.20    8.78    0.83    2.85   5.75    2.04   \n",
       "4       0.00       2.45   6.09  10.30    7.23    1.92    3.54   7.17    1.42   \n",
       "\n",
       "   653553  ...  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B    ZYX  ZZEF1  ZZZ3  \\\n",
       "0   10.49  ...  4.44  8.46  10.04    0.57    9.34  10.85  10.18  9.22   \n",
       "1    7.62  ...  5.86  8.13  11.54    5.02   10.19  11.58  10.89  9.65   \n",
       "2    9.59  ...  5.35  8.96   9.84    0.67    9.66  11.38  10.53  8.78   \n",
       "3   10.25  ...  4.23  7.69   9.80    3.66    9.12  11.21  10.16  9.01   \n",
       "4    8.70  ...  3.79  6.89   9.81    3.14    9.64   9.47   9.64  8.90   \n",
       "\n",
       "                           Subtype  Subtype_encoded  \n",
       "0  Lymphocyte Depleted (Immune C4)                4  \n",
       "1         Inflammatory (Immune C3)                3  \n",
       "2  Lymphocyte Depleted (Immune C4)                4  \n",
       "3  Lymphocyte Depleted (Immune C4)                4  \n",
       "4  Lymphocyte Depleted (Immune C4)                4  \n",
       "\n",
       "[5 rows x 16337 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Create a MinMaxScaler object\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit the scaler to the training data and transform it\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Transform the test data\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert numpy arrays to pandas DataFrames\n",
    "# X_train = pd.DataFrame(X_train_scaled)\n",
    "# X_test = pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      100133144  100134869  10357  10431  155060  388795  390284  57714  \\\n",
      "2983       3.51       2.29   7.54  10.85    8.74    2.97    2.57   7.92   \n",
      "4598       2.58       2.31   6.85   9.35    7.28    0.00    2.73   9.48   \n",
      "5067       4.05       1.82   6.49   9.73    6.89    0.35    2.27   8.78   \n",
      "6927       2.51       4.39   6.84  10.34    6.07    0.00    3.31  10.15   \n",
      "5921       0.85       3.76   7.30   9.30    8.29    1.49    2.83   9.54   \n",
      "\n",
      "      645851  653553  ...  ZWILCH  ZWINT  ZXDA   ZXDB   ZXDC  ZYG11A  ZYG11B  \\\n",
      "2983    5.13   12.11  ...    9.50  11.33  5.28   7.89  10.63    3.18    8.50   \n",
      "4598    4.04    8.83  ...    8.53   9.84  6.42   9.93  10.64    5.87   10.08   \n",
      "5067    3.85    8.32  ...    9.84   9.53  6.59  10.11   9.72    8.02    9.98   \n",
      "6927    2.64    6.66  ...    8.99   9.34  5.53   9.22  10.73    2.05    9.47   \n",
      "5921    3.88    7.00  ...    7.24   6.65  6.16   8.66  10.48    4.69   11.12   \n",
      "\n",
      "        ZYX  ZZEF1   ZZZ3  \n",
      "2983  12.56   9.23   8.39  \n",
      "4598  12.19  10.79  10.20  \n",
      "5067  11.79   9.53   9.88  \n",
      "6927  11.36   9.31   9.76  \n",
      "5921  11.46  10.76  10.49  \n",
      "\n",
      "[5 rows x 16335 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])  # This will print the first 5 entries of X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100133144</th>\n",
       "      <th>100134869</th>\n",
       "      <th>10357</th>\n",
       "      <th>10431</th>\n",
       "      <th>155060</th>\n",
       "      <th>388795</th>\n",
       "      <th>390284</th>\n",
       "      <th>57714</th>\n",
       "      <th>645851</th>\n",
       "      <th>653553</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>3.51</td>\n",
       "      <td>2.29</td>\n",
       "      <td>7.54</td>\n",
       "      <td>10.85</td>\n",
       "      <td>8.74</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.57</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5.13</td>\n",
       "      <td>12.11</td>\n",
       "      <td>...</td>\n",
       "      <td>9.50</td>\n",
       "      <td>11.33</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.89</td>\n",
       "      <td>10.63</td>\n",
       "      <td>3.18</td>\n",
       "      <td>8.50</td>\n",
       "      <td>12.56</td>\n",
       "      <td>9.23</td>\n",
       "      <td>8.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>2.58</td>\n",
       "      <td>2.31</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.35</td>\n",
       "      <td>7.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.48</td>\n",
       "      <td>4.04</td>\n",
       "      <td>8.83</td>\n",
       "      <td>...</td>\n",
       "      <td>8.53</td>\n",
       "      <td>9.84</td>\n",
       "      <td>6.42</td>\n",
       "      <td>9.93</td>\n",
       "      <td>10.64</td>\n",
       "      <td>5.87</td>\n",
       "      <td>10.08</td>\n",
       "      <td>12.19</td>\n",
       "      <td>10.79</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>4.05</td>\n",
       "      <td>1.82</td>\n",
       "      <td>6.49</td>\n",
       "      <td>9.73</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>8.78</td>\n",
       "      <td>3.85</td>\n",
       "      <td>8.32</td>\n",
       "      <td>...</td>\n",
       "      <td>9.84</td>\n",
       "      <td>9.53</td>\n",
       "      <td>6.59</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.72</td>\n",
       "      <td>8.02</td>\n",
       "      <td>9.98</td>\n",
       "      <td>11.79</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6927</th>\n",
       "      <td>2.51</td>\n",
       "      <td>4.39</td>\n",
       "      <td>6.84</td>\n",
       "      <td>10.34</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.31</td>\n",
       "      <td>10.15</td>\n",
       "      <td>2.64</td>\n",
       "      <td>6.66</td>\n",
       "      <td>...</td>\n",
       "      <td>8.99</td>\n",
       "      <td>9.34</td>\n",
       "      <td>5.53</td>\n",
       "      <td>9.22</td>\n",
       "      <td>10.73</td>\n",
       "      <td>2.05</td>\n",
       "      <td>9.47</td>\n",
       "      <td>11.36</td>\n",
       "      <td>9.31</td>\n",
       "      <td>9.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>0.85</td>\n",
       "      <td>3.76</td>\n",
       "      <td>7.30</td>\n",
       "      <td>9.30</td>\n",
       "      <td>8.29</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.83</td>\n",
       "      <td>9.54</td>\n",
       "      <td>3.88</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.65</td>\n",
       "      <td>6.16</td>\n",
       "      <td>8.66</td>\n",
       "      <td>10.48</td>\n",
       "      <td>4.69</td>\n",
       "      <td>11.12</td>\n",
       "      <td>11.46</td>\n",
       "      <td>10.76</td>\n",
       "      <td>10.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16335 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      100133144  100134869  10357  10431  155060  388795  390284  57714  \\\n",
       "2983       3.51       2.29   7.54  10.85    8.74    2.97    2.57   7.92   \n",
       "4598       2.58       2.31   6.85   9.35    7.28    0.00    2.73   9.48   \n",
       "5067       4.05       1.82   6.49   9.73    6.89    0.35    2.27   8.78   \n",
       "6927       2.51       4.39   6.84  10.34    6.07    0.00    3.31  10.15   \n",
       "5921       0.85       3.76   7.30   9.30    8.29    1.49    2.83   9.54   \n",
       "\n",
       "      645851  653553  ...  ZWILCH  ZWINT  ZXDA   ZXDB   ZXDC  ZYG11A  ZYG11B  \\\n",
       "2983    5.13   12.11  ...    9.50  11.33  5.28   7.89  10.63    3.18    8.50   \n",
       "4598    4.04    8.83  ...    8.53   9.84  6.42   9.93  10.64    5.87   10.08   \n",
       "5067    3.85    8.32  ...    9.84   9.53  6.59  10.11   9.72    8.02    9.98   \n",
       "6927    2.64    6.66  ...    8.99   9.34  5.53   9.22  10.73    2.05    9.47   \n",
       "5921    3.88    7.00  ...    7.24   6.65  6.16   8.66  10.48    4.69   11.12   \n",
       "\n",
       "        ZYX  ZZEF1   ZZZ3  \n",
       "2983  12.56   9.23   8.39  \n",
       "4598  12.19  10.79  10.20  \n",
       "5067  11.79   9.53   9.88  \n",
       "6927  11.36   9.31   9.76  \n",
       "5921  11.46  10.76  10.49  \n",
       "\n",
       "[5 rows x 16335 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "df.head()  # Now you can use the head() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84531113 0.83735204 0.82639193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# # Cross-validation\n",
    "# clf = LogisticRegression(max_iter=200)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.89      0.89       657\n",
      "           2       0.86      0.89      0.88        84\n",
      "           3       0.86      0.90      0.88       609\n",
      "           4       0.78      0.73      0.76       293\n",
      "           5       0.27      0.19      0.23        47\n",
      "           6       0.83      0.84      0.84       592\n",
      "\n",
      "    accuracy                           0.85      2282\n",
      "   macro avg       0.75      0.74      0.74      2282\n",
      "weighted avg       0.84      0.85      0.84      2282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# # Select the best features\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "# fit = bestfeatures.fit(X_train, y_train)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfcolumns = pd.DataFrame(X_train.columns)\n",
    "# # concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "# featureScores.columns = ['Features', 'Score'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Select the best features\n",
    "bestfeatures = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "fit = bestfeatures.fit(X_train, y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "\n",
    "# concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Features', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>CEP55</td>\n",
       "      <td>0.460449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>CCNB1</td>\n",
       "      <td>0.440509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4078</th>\n",
       "      <td>DLGAP5</td>\n",
       "      <td>0.439720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>CDC20</td>\n",
       "      <td>0.435409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>CCNB2</td>\n",
       "      <td>0.432219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>RAD51</td>\n",
       "      <td>0.431940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7397</th>\n",
       "      <td>KIF2C</td>\n",
       "      <td>0.423997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10699</th>\n",
       "      <td>PLK1</td>\n",
       "      <td>0.423727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CENPW</td>\n",
       "      <td>0.422179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>CDC6</td>\n",
       "      <td>0.419366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>CENPA</td>\n",
       "      <td>0.418466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>CDCA8</td>\n",
       "      <td>0.417929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14895</th>\n",
       "      <td>TUBA1C</td>\n",
       "      <td>0.416588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9158</th>\n",
       "      <td>MYBL2</td>\n",
       "      <td>0.415946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14167</th>\n",
       "      <td>TK1</td>\n",
       "      <td>0.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>KIF23</td>\n",
       "      <td>0.413216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>CDCA4</td>\n",
       "      <td>0.411364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>MELK</td>\n",
       "      <td>0.410933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>CDK1</td>\n",
       "      <td>0.410930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>ERCC6L</td>\n",
       "      <td>0.408592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9478</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>0.405513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>UBE2C</td>\n",
       "      <td>0.404750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14744</th>\n",
       "      <td>TROAP</td>\n",
       "      <td>0.404675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>FAM54A</td>\n",
       "      <td>0.404266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>CDCA5</td>\n",
       "      <td>0.402771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>TPX2</td>\n",
       "      <td>0.401952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7508</th>\n",
       "      <td>KPNA2</td>\n",
       "      <td>0.401135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14725</th>\n",
       "      <td>TRIP13</td>\n",
       "      <td>0.399459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>KIF20A</td>\n",
       "      <td>0.399302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9334</th>\n",
       "      <td>NCAPH</td>\n",
       "      <td>0.398885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Features     Score\n",
       "2980     CEP55  0.460449\n",
       "2662     CCNB1  0.440509\n",
       "4078    DLGAP5  0.439720\n",
       "2805     CDC20  0.435409\n",
       "2664     CCNB2  0.432219\n",
       "11507    RAD51  0.431940\n",
       "7397     KIF2C  0.423997\n",
       "10699     PLK1  0.423727\n",
       "2968     CENPW  0.422179\n",
       "2828      CDC6  0.419366\n",
       "2950     CENPA  0.418466\n",
       "2837     CDCA8  0.417929\n",
       "14895   TUBA1C  0.416588\n",
       "9158     MYBL2  0.415946\n",
       "14167      TK1  0.413500\n",
       "7391     KIF23  0.413216\n",
       "2833     CDCA4  0.411364\n",
       "8653      MELK  0.410933\n",
       "2869      CDK1  0.410930\n",
       "4674    ERCC6L  0.408592\n",
       "9478      NEK2  0.405513\n",
       "14991    UBE2C  0.404750\n",
       "14744    TROAP  0.404675\n",
       "5020    FAM54A  0.404266\n",
       "2834     CDCA5  0.402771\n",
       "14616     TPX2  0.401952\n",
       "7508     KPNA2  0.401135\n",
       "14725   TRIP13  0.399459\n",
       "7386    KIF20A  0.399302\n",
       "9334     NCAPH  0.398885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featureScores.sort_values('Score', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6844, 500), (2282, 500))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select optimal features\n",
    "X_optimal = X_train[featureScores.sort_values('Score', ascending=False).Features[0:500]]\n",
    "X_tst_optimal = X_test[featureScores.sort_values('Score', ascending=False).Features[0:500]]\n",
    "X_optimal.shape, X_tst_optimal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84662577 0.83779044 0.82113108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "print(cross_val_score(clf, X_optimal, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.87      0.89       657\n",
      "           2       0.94      0.90      0.92        84\n",
      "           3       0.85      0.92      0.88       609\n",
      "           4       0.78      0.75      0.76       293\n",
      "           5       0.45      0.28      0.34        47\n",
      "           6       0.81      0.83      0.82       592\n",
      "\n",
      "    accuracy                           0.84      2282\n",
      "   macro avg       0.79      0.76      0.77      2282\n",
      "weighted avg       0.84      0.84      0.84      2282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_optimal, y_train)\n",
    "y_pred = clf.predict(X_tst_optimal)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8330411919368974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.91      0.89       657\n",
      "           2       0.94      0.89      0.91        84\n",
      "           3       0.82      0.91      0.87       609\n",
      "           4       0.82      0.62      0.70       293\n",
      "           5       0.33      0.02      0.04        47\n",
      "           6       0.80      0.83      0.81       592\n",
      "\n",
      "    accuracy                           0.83      2282\n",
      "   macro avg       0.76      0.70      0.70      2282\n",
      "weighted avg       0.82      0.83      0.82      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf_rf.fit(X_optimal, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred_rf = clf_rf.predict(X_tst_optimal)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN, Logistic Regression, DT, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier Accuracy: 0.7743207712532866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.81       657\n",
      "           2       0.87      0.92      0.89        84\n",
      "           3       0.79      0.91      0.84       609\n",
      "           4       0.79      0.54      0.64       293\n",
      "           5       0.19      0.11      0.14        47\n",
      "           6       0.72      0.75      0.73       592\n",
      "\n",
      "    accuracy                           0.77      2282\n",
      "   macro avg       0.69      0.67      0.68      2282\n",
      "weighted avg       0.77      0.77      0.77      2282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.8448729184925504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.87      0.89       657\n",
      "           2       0.94      0.90      0.92        84\n",
      "           3       0.85      0.92      0.88       609\n",
      "           4       0.78      0.75      0.76       293\n",
      "           5       0.45      0.28      0.34        47\n",
      "           6       0.81      0.83      0.82       592\n",
      "\n",
      "    accuracy                           0.84      2282\n",
      "   macro avg       0.79      0.76      0.77      2282\n",
      "weighted avg       0.84      0.84      0.84      2282\n",
      "\n",
      "DecisionTreeClassifier Accuracy: 0.7287467134092901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.79      0.80       657\n",
      "           2       0.86      0.83      0.85        84\n",
      "           3       0.78      0.79      0.78       609\n",
      "           4       0.54      0.60      0.57       293\n",
      "           5       0.18      0.19      0.19        47\n",
      "           6       0.72      0.69      0.70       592\n",
      "\n",
      "    accuracy                           0.73      2282\n",
      "   macro avg       0.65      0.65      0.65      2282\n",
      "weighted avg       0.73      0.73      0.73      2282\n",
      "\n",
      "SVM Accuracy: 0.8492550394390885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.89      0.90       657\n",
      "           2       0.90      0.89      0.90        84\n",
      "           3       0.84      0.92      0.88       609\n",
      "           4       0.83      0.70      0.76       293\n",
      "           5       0.00      0.00      0.00        47\n",
      "           6       0.81      0.86      0.83       592\n",
      "\n",
      "    accuracy                           0.85      2282\n",
      "   macro avg       0.71      0.71      0.71      2282\n",
      "weighted avg       0.83      0.85      0.84      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"SVM\": svm.SVC()\n",
    "}\n",
    "\n",
    "# Train each of the models and evaluate their performance\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the model\n",
    "    clf.fit(X_optimal, y_train)\n",
    "    \n",
    "    # Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_tst_optimal)\n",
    "    \n",
    "    # Model Accuracy\n",
    "    print(f\"{name} Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost & Light GBM with X and y optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127493\n",
      "[LightGBM] [Info] Number of data points in the train set: 6844, number of used features: 500\n",
      "[LightGBM] [Info] Start training from score -1.263782\n",
      "[LightGBM] [Info] Start training from score -3.124017\n",
      "[LightGBM] [Info] Start training from score -1.342275\n",
      "[LightGBM] [Info] Start training from score -2.069555\n",
      "[LightGBM] [Info] Start training from score -3.940779\n",
      "[LightGBM] [Info] Start training from score -1.322340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters for XGBoost:  {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "Best parameters for LightGBM:  {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.2}\n",
      "Test set score for XGBoost:  0.8694127957931639\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test set score for LightGBM:  0.8755477651183172\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the models\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Define the hyperparameters for RandomizedSearchCV\n",
    "params_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [None, 5, 10, 15]\n",
    "}\n",
    "\n",
    "params_lgbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [None, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "random_search_xgb = RandomizedSearchCV(xgb, params_xgb, cv=3, n_jobs=-1, verbose=1)\n",
    "random_search_xgb.fit(X_optimal, y_train - 1)\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm, params_lgbm, cv=3, n_jobs=-1, verbose=1)\n",
    "random_search_lgbm.fit(X_optimal, y_train - 1)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters for XGBoost: \", random_search_xgb.best_params_)\n",
    "print(\"Best parameters for LightGBM: \", random_search_lgbm.best_params_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(\"Test set score for XGBoost: \", random_search_xgb.score(X_tst_optimal, y_test - 1))\n",
    "print(\"Test set score for LightGBM: \", random_search_lgbm.score(X_tst_optimal, y_test - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Classification report for XGBoost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       657\n",
      "           1       0.93      0.90      0.92        84\n",
      "           2       0.87      0.93      0.90       609\n",
      "           3       0.85      0.75      0.79       293\n",
      "           4       0.44      0.15      0.22        47\n",
      "           5       0.83      0.88      0.85       592\n",
      "\n",
      "    accuracy                           0.87      2282\n",
      "   macro avg       0.81      0.75      0.77      2282\n",
      "weighted avg       0.86      0.87      0.86      2282\n",
      "\n",
      "Classification report for LightGBM: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       657\n",
      "           1       0.94      0.90      0.92        84\n",
      "           2       0.89      0.93      0.91       609\n",
      "           3       0.85      0.76      0.80       293\n",
      "           4       0.33      0.15      0.21        47\n",
      "           5       0.84      0.89      0.86       592\n",
      "\n",
      "    accuracy                           0.88      2282\n",
      "   macro avg       0.79      0.76      0.77      2282\n",
      "weighted avg       0.87      0.88      0.87      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_xgb = random_search_xgb.predict(X_tst_optimal)\n",
    "y_pred_lgbm = random_search_lgbm.predict(X_tst_optimal)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report for XGBoost: \")\n",
    "print(classification_report(y_test - 1, y_pred_xgb))\n",
    "\n",
    "print(\"Classification report for LightGBM: \")\n",
    "print(classification_report(y_test - 1, y_pred_lgbm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost accuracy:  0.8755477651183172\n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a model\n",
    "# xgb_model = XGBClassifier()\n",
    "\n",
    "# # Train the model\n",
    "# xgb_model.fit(X_train, y_train - 1)  # Subtract 1 here\n",
    "\n",
    "# # Predict the classes\n",
    "# y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(\"XGBoost accuracy: \", accuracy_score(y_test - 1, y_pred))  # Subtract 1 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92       657\n",
      "           2       0.94      0.90      0.92        84\n",
      "           3       0.87      0.91      0.89       609\n",
      "           4       0.85      0.75      0.80       293\n",
      "           5       0.41      0.15      0.22        47\n",
      "           6       0.85      0.90      0.87       592\n",
      "\n",
      "    accuracy                           0.88      2282\n",
      "   macro avg       0.81      0.76      0.77      2282\n",
      "weighted avg       0.87      0.88      0.87      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# # Predict the classes\n",
    "# y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# # Convert predictions into original scale\n",
    "# y_pred_classes = y_pred + 1  # Add 1 here if your classes start from 1\n",
    "\n",
    "# # Print the classification report\n",
    "# print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_23116\\559140858.py:2: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  X_train.columns = X_train.columns.str.replace('[{}[\\]:,\"]', '', regex=True)\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_23116\\559140858.py:3: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  X_test.columns = X_test.columns.str.replace('[{}[\\]:,\"]', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.033695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4159940\n",
      "[LightGBM] [Info] Number of data points in the train set: 6844, number of used features: 16335\n",
      "[LightGBM] [Info] Start training from score -1.263782\n",
      "[LightGBM] [Info] Start training from score -3.124017\n",
      "[LightGBM] [Info] Start training from score -1.342275\n",
      "[LightGBM] [Info] Start training from score -2.069555\n",
      "[LightGBM] [Info] Start training from score -3.940779\n",
      "[LightGBM] [Info] Start training from score -1.322340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM accuracy:  0.8821209465381245\n"
     ]
    }
   ],
   "source": [
    "# # Replace special characters in column names\n",
    "# X_train.columns = X_train.columns.str.replace('[{}[\\]:,\"]', '', regex=True)\n",
    "# X_test.columns = X_test.columns.str.replace('[{}[\\]:,\"]', '', regex=True)\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a model\n",
    "# lgb_model = LGBMClassifier()\n",
    "\n",
    "# # Train the model\n",
    "# lgb_model.fit(X_train, y_train - 1)  # Subtract 1 here\n",
    "\n",
    "# # Predict the classes\n",
    "# y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(\"LightGBM accuracy: \", accuracy_score(y_test - 1, y_pred))  # Subtract 1 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.93      0.93       657\n",
      "           2       0.95      0.89      0.92        84\n",
      "           3       0.88      0.93      0.90       609\n",
      "           4       0.86      0.76      0.81       293\n",
      "           5       0.50      0.06      0.11        47\n",
      "           6       0.85      0.90      0.88       592\n",
      "\n",
      "    accuracy                           0.88      2282\n",
      "   macro avg       0.83      0.75      0.76      2282\n",
      "weighted avg       0.88      0.88      0.87      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Convert predictions into original scale\n",
    "# y_pred_classes = y_pred + 1  # Add 1 here if your classes start from 1\n",
    "\n",
    "# # Print the classification report\n",
    "# print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define base learners\n",
    "base_learners = [\n",
    "                 ('rf', RandomForestClassifier()),\n",
    "                 ('knn', KNeighborsClassifier()),\n",
    "                 ('lr', LogisticRegression()),\n",
    "                 ('dt', DecisionTreeClassifier()),\n",
    "                 ('svm', SVC()),\n",
    "                 ('xgb', XGBClassifier()),\n",
    "                 ('lgbm', LGBMClassifier())\n",
    "                ]\n",
    "\n",
    "# Initialize StackingClassifier with the Meta Learner\n",
    "stack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "\n",
    "# Parameters for RandomizedSearchCV\n",
    "params = {\n",
    "          'rf__n_estimators': [50, 100, 200],\n",
    "          'rf__max_depth': [None, 5, 10, 15],\n",
    "          'knn__n_neighbors': [3, 5, 7, 9],\n",
    "          'lr__C': [0.1, 1.0, 10.0],\n",
    "          'dt__max_depth': [None, 5, 10, 15],\n",
    "          'svm__C': [0.1, 1.0, 10.0],\n",
    "          'xgb__n_estimators': [50, 100, 200],\n",
    "          'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "          'lgbm__n_estimators': [50, 100, 200],\n",
    "          'lgbm__learning_rate': [0.01, 0.1, 0.2]\n",
    "         }\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "random_search = RandomizedSearchCV(stack, params, cv=3, n_jobs=-1, verbose=1)\n",
    "random_search.fit(X_optimal, y_train - 1)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(\"Test set score: \", random_search.score(X_tst_optimal, y_test - 1))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = random_search.predict(X_tst_optimal)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test - 1, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = DecisionTreeClassifier()\n",
    "clf5 = SVC()\n",
    "clf6 = XGBClassifier()\n",
    "clf7 = LGBMClassifier()\n",
    "\n",
    "# Create the ensemble model\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('rf', clf1), ('knn', clf2), ('lr', clf3), ('dt', clf4), ('svm', clf5), ('xgb', clf6), ('lgbm', clf7)],\n",
    "    voting='hard')\n",
    "\n",
    "eclf.fit(X_optimal, y_train - 1)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = eclf.predict(X_tst_optimal)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test - 1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[30], line 40\u001b[0m\n",
      "\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter optimization\u001b[39;00m\n",
      "\u001b[0;32m     39\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(stack, params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m---> 40\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X_optimal, y_train)\n",
      "\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Print the best parameters\u001b[39;00m\n",
      "\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n",
      "\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n",
      "\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n",
      "\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n",
      "\u001b[0;32m   1471\u001b[0m     )\n",
      "\u001b[0;32m   1472\u001b[0m ):\n",
      "\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n",
      "\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n",
      "\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n",
      "\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n",
      "\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n",
      "\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1930\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n",
      "\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n",
      "\u001b[0;32m   1929\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 1930\u001b[0m     evaluate_candidates(\n",
      "\u001b[0;32m   1931\u001b[0m         ParameterSampler(\n",
      "\u001b[0;32m   1932\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n",
      "\u001b[0;32m   1933\u001b[0m         )\n",
      "\u001b[0;32m   1934\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n",
      "\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n",
      "\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n",
      "\u001b[0;32m    911\u001b[0m         )\n",
      "\u001b[0;32m    912\u001b[0m     )\n",
      "\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n",
      "\u001b[0;32m    915\u001b[0m     delayed(_fit_and_score)(\n",
      "\u001b[0;32m    916\u001b[0m         clone(base_estimator),\n",
      "\u001b[0;32m    917\u001b[0m         X,\n",
      "\u001b[0;32m    918\u001b[0m         y,\n",
      "\u001b[0;32m    919\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n",
      "\u001b[0;32m    920\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n",
      "\u001b[0;32m    921\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n",
      "\u001b[0;32m    922\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n",
      "\u001b[0;32m    923\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n",
      "\u001b[0;32m    924\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n",
      "\u001b[0;32m    925\u001b[0m     )\n",
      "\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n",
      "\u001b[0;32m    927\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n",
      "\u001b[0;32m    928\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n",
      "\u001b[0;32m    929\u001b[0m     )\n",
      "\u001b[0;32m    930\u001b[0m )\n",
      "\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    937\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n",
      "\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n",
      "\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n",
      "\u001b[0;32m     66\u001b[0m )\n",
      "\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n",
      "\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n",
      "\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n",
      "\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n",
      "\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n",
      "\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n",
      "\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n",
      "\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n",
      "\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# # Define base learners\n",
    "# base_learners = [\n",
    "#                  ('rf', RandomForestClassifier()),\n",
    "#                  ('knn', KNeighborsClassifier()),\n",
    "#                  ('lr', LogisticRegression()),\n",
    "#                  ('dt', DecisionTreeClassifier()),\n",
    "#                  ('svm', SVC()),\n",
    "#                  ('xgb', XGBClassifier()),\n",
    "#                  ('lgbm', LGBMClassifier())\n",
    "#                 ]\n",
    "\n",
    "# # Initialize StackingClassifier with the Meta Learner\n",
    "# stack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "\n",
    "# # Parameters for RandomizedSearchCV\n",
    "# params = {\n",
    "#           'rf__n_estimators': [50, 100, 200],\n",
    "#           'rf__max_depth': [None, 5, 10, 15],\n",
    "#           'knn__n_neighbors': [3, 5, 7, 9],\n",
    "#           'lr__C': [0.1, 1.0, 10.0],\n",
    "#           'dt__max_depth': [None, 5, 10, 15],\n",
    "#           'svm__C': [0.1, 1.0, 10.0],\n",
    "#           'xgb__n_estimators': [50, 100, 200],\n",
    "#           'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#           'lgbm__n_estimators': [50, 100, 200],\n",
    "#           'lgbm__learning_rate': [0.01, 0.1, 0.2]\n",
    "#          }\n",
    "\n",
    "# # Perform hyperparameter optimization\n",
    "# random_search = RandomizedSearchCV(stack, params, cv=3, n_jobs=-1, verbose=1)\n",
    "# random_search.fit(X_optimal, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters: \", random_search.best_params_)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# print(\"Test set score: \", random_search.score(X_tst_optimal, y_test))\n",
    "# from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# # Define base learners\n",
    "# base_learners = [\n",
    "#                  ('rf', RandomForestClassifier()),\n",
    "#                  ('knn', KNeighborsClassifier()),\n",
    "#                  ('lr', LogisticRegression()),\n",
    "#                  ('dt', DecisionTreeClassifier()),\n",
    "#                  ('svm', SVC()),\n",
    "#                  ('xgb', XGBClassifier()),\n",
    "#                  ('lgbm', LGBMClassifier())\n",
    "#                 ]\n",
    "\n",
    "# # Initialize StackingClassifier with the Meta Learner\n",
    "# stack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "\n",
    "# # Parameters for RandomizedSearchCV\n",
    "# params = {\n",
    "#           'rf__n_estimators': [50, 100, 200],\n",
    "#           'rf__max_depth': [None, 5, 10, 15],\n",
    "#           'knn__n_neighbors': [3, 5, 7, 9],\n",
    "#           'lr__C': [0.1, 1.0, 10.0],\n",
    "#           'dt__max_depth': [None, 5, 10, 15],\n",
    "#           'svm__C': [0.1, 1.0, 10.0],\n",
    "#           'xgb__n_estimators': [50, 100, 200],\n",
    "#           'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#           'lgbm__n_estimators': [50, 100, 200],\n",
    "#           'lgbm__learning_rate': [0.01, 0.1, 0.2]\n",
    "#          }\n",
    "\n",
    "# # Perform hyperparameter optimization\n",
    "# random_search = RandomizedSearchCV(stack, params, cv=3, n_jobs=-1, verbose=1)\n",
    "# random_search.fit(X_optimal, y_train)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters: \", random_search.best_params_)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# print(\"Test set score: \", random_search.score(X_tst_optimal, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the base models\n",
    "# base_models = [\n",
    "#     ('rf', Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier())])),\n",
    "#     ('knn', Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])),\n",
    "#     ('lr', Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=200))])),\n",
    "#     ('dt', Pipeline([('scaler', StandardScaler()), ('dt', DecisionTreeClassifier())])),\n",
    "#     ('svm', Pipeline([('scaler', StandardScaler()), ('svm', svm.SVC(probability=True))])),\n",
    "#     ('xgb', Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))])),\n",
    "#     ('lgm', Pipeline([('scaler', StandardScaler()), ('lgm', LGBMClassifier())]))\n",
    "# ]\n",
    "\n",
    "# # Define the meta model\n",
    "# meta_model = LogisticRegression()\n",
    "\n",
    "# # Define the stacking classifier\n",
    "# stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# # Define the parameter grid for each base model\n",
    "# param_grid = {\n",
    "#     'rf__rf__n_estimators': [50, 100, 200],\n",
    "#     'knn__knn__n_neighbors': [3, 5, 7],\n",
    "#     'lr__lr__C': [0.1, 1.0, 10.0],\n",
    "#     'dt__dt__max_depth': [None, 5, 10],\n",
    "#     'svm__svm__C': [0.1, 1.0, 10.0],\n",
    "#     'xgb__xgb__n_estimators': [50, 100, 200],\n",
    "#     'lgm__lgm__n_estimators': [50, 100, 200]\n",
    "# }\n",
    "\n",
    "# # Define the grid search\n",
    "# grid = GridSearchCV(estimator=stacking_model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# # Fit the grid search\n",
    "# grid.fit(X_train, y_train - 1)  # Subtract 1 here\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "# # Predict the classes\n",
    "# y_pred = grid.predict(X_test)\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(\"Stacking model accuracy: \", accuracy_score(y_test - 1, y_pred))  # Subtract 1 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import svm\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Define the parameter grids for each classifier\n",
    "# param_grids = {\n",
    "#     'rf__n_estimators': [50, 100, 200],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'rf__min_samples_split': [2, 5, 10],\n",
    "#     'knn__n_neighbors': [3, 5, 7, 9],\n",
    "#     'knn__weights': ['uniform', 'distance'],\n",
    "#     'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "#     'lr__solver': ['lbfgs', 'liblinear'],\n",
    "#     'dt__max_depth': [None, 10, 20, 30],\n",
    "#     'dt__min_samples_split': [2, 5, 10],\n",
    "#     'svm__C': [0.01, 0.1, 1, 10, 100],\n",
    "#     'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'svm__gamma': ['scale', 'auto'],\n",
    "#     'xgb__n_estimators': [50, 100, 200],\n",
    "#     'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'xgb__max_depth': [3, 6, 9],\n",
    "#     'lgm__n_estimators': [50, 100, 200],\n",
    "#     'lgm__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'lgm__max_depth': [3, 6, 9]\n",
    "# }\n",
    "\n",
    "# # Define the classifiers with pipeline for scaling\n",
    "# pipelines = {\n",
    "#     'rf': Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier())]),\n",
    "#     'knn': Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),\n",
    "#     'lr': Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=200))]),\n",
    "#     'dt': Pipeline([('scaler', StandardScaler()), ('dt', DecisionTreeClassifier())]),\n",
    "#     'svm': Pipeline([('scaler', StandardScaler()), ('svm', svm.SVC(probability=True))]),\n",
    "#     'xgb': Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))]),\n",
    "#     'lgm': Pipeline([('scaler', StandardScaler()), ('lgm', LGBMClassifier())])\n",
    "# }\n",
    "\n",
    "# # Function to perform RandomizedSearchCV for a given classifier\n",
    "# def randomized_search_cv(pipeline, param_grid, X_train, y_train):\n",
    "#     search = RandomizedSearchCV(pipeline, {key: param_grid[key] for key in param_grid if key.startswith(pipeline.steps[-1][0] + '__')}, \n",
    "#                                 cv=3, n_iter=10, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "#     search.fit(X_train, y_train)\n",
    "#     return search.best_estimator_\n",
    "\n",
    "# # Assuming featureScores is a DataFrame with columns 'Features' and 'Score'\n",
    "# # Select optimal features\n",
    "# X_optimal = X_train[featureScores.sort_values('Score', ascending=False).Features[0:500]]\n",
    "# X_tst_optimal = X_test[featureScores.sort_values('Score', ascending=False).Features[0:500]]\n",
    "\n",
    "# # Adjust the target variable\n",
    "# y_train_adjusted = y_train - 1\n",
    "# y_test_adjusted = y_test - 1\n",
    "\n",
    "# # Optimize each classifier\n",
    "# best_classifiers = {}\n",
    "# for name, pipeline in pipelines.items():\n",
    "#     if name == 'xgb':\n",
    "#         best_classifiers[name] = randomized_search_cv(pipeline, param_grids, X_optimal, y_train_adjusted)\n",
    "#     else:\n",
    "#         best_classifiers[name] = randomized_search_cv(pipeline, param_grids, X_optimal, y_train)\n",
    "\n",
    "# # Define the base estimators for voting\n",
    "# base_estimators = [(name, clf) for name, clf in best_classifiers.items()]\n",
    "\n",
    "# # Soft Voting Classifier\n",
    "# voting_clf_soft = VotingClassifier(estimators=base_estimators, voting='soft')\n",
    "# voting_clf_soft.fit(X_optimal, y_train_adjusted)\n",
    "# soft_predictions = voting_clf_soft.predict(X_tst_optimal)\n",
    "# soft_accuracy = accuracy_score(y_test_adjusted, soft_predictions)\n",
    "# print(\"Soft Voting Classifier Accuracy:\", soft_accuracy)\n",
    "# print(\"Soft Voting Classifier Classification Report:\")\n",
    "# print(classification_report(y_test_adjusted, soft_predictions))\n",
    "\n",
    "# # Hard Voting Classifier\n",
    "# voting_clf_hard = VotingClassifier(estimators=base_estimators, voting='hard')\n",
    "# voting_clf_hard.fit(X_optimal, y_train_adjusted)\n",
    "# hard_predictions = voting_clf_hard.predict(X_tst_optimal)\n",
    "# hard_accuracy = accuracy_score(y_test_adjusted, hard_predictions)\n",
    "# print(\"Hard Voting Classifier Accuracy:\", hard_accuracy)\n",
    "# print(\"Hard Voting Classifier Classification Report:\")\n",
    "# print(classification_report(y_test_adjusted, hard_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('lr', LogisticRegression(max_iter=200)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svm', svm.SVC()),\n",
    "    ('xgb', XGBClassifier()),  # Add XGBoost here\n",
    "    'lgm', LGBMClassifier())\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.8641542506573181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91       665\n",
      "           2       0.92      0.94      0.93        96\n",
      "           3       0.85      0.92      0.88       575\n",
      "           4       0.83      0.75      0.79       289\n",
      "           5       0.41      0.20      0.27        45\n",
      "           6       0.85      0.87      0.86       612\n",
      "\n",
      "    accuracy                           0.86      2282\n",
      "   macro avg       0.79      0.76      0.77      2282\n",
      "weighted avg       0.86      0.86      0.86      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=classifiers, final_estimator=LogisticRegression())\n",
    "\n",
    "# Train the classifier\n",
    "stacking_clf.fit(X_optimal, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = stacking_clf.predict(X_tst_optimal)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\anaconda3\\envs\\Hannah\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.4296 - loss: 163.2504\n",
      "Epoch 2/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.6577 - loss: 8.6109\n",
      "Epoch 3/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.7794 - loss: 1.9277\n",
      "Epoch 4/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8532 - loss: 0.6118\n",
      "Epoch 5/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9037 - loss: 0.3283\n",
      "Epoch 6/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9492 - loss: 0.1602\n",
      "Epoch 7/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9465 - loss: 0.1424\n",
      "Epoch 8/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9839 - loss: 0.0519\n",
      "Epoch 9/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9830 - loss: 0.0828\n",
      "Epoch 10/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9831 - loss: 0.0537\n",
      "Epoch 11/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9791 - loss: 0.0713\n",
      "Epoch 12/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9966 - loss: 0.0182\n",
      "Epoch 13/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9958 - loss: 0.0168\n",
      "Epoch 14/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9567 - loss: 0.1588\n",
      "Epoch 15/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9833 - loss: 0.0591\n",
      "Epoch 16/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9962 - loss: 0.0159\n",
      "Epoch 17/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0034\n",
      "Epoch 18/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 19/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 9.7470e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.2030e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.8806e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.1843e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.0488e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.3306e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8940e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4175e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.8688e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.4922e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.1505e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.0066e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.6653e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.5181e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.3919e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.2235e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.0427e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.7060e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.1144e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.2598e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.0336e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.7680e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.8388e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.5177e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.7187e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.1902e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.0641e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.3606e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.2590e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.9588e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.7173e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.3272e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.2481e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1896e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0002e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.7101e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.5362e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.8348e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.6178e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.0028e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.7946e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.0700e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.6215e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.2429e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.7702e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.3560e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.0732e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.9315e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.7278e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.5447e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.2927e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.1905e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0718e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0027e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.5216e-07\n",
      "Epoch 74/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.5071e-07\n",
      "Epoch 75/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.5947e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.9275e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.8983e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.0286e-07\n",
      "Epoch 79/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.2539e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8794e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.4611e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.9064e-07\n",
      "Epoch 83/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.7339e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.6974e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.2768e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.0554e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.9218e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.7147e-07\n",
      "Epoch 89/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.6365e-07\n",
      "Epoch 90/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.4504e-07\n",
      "Epoch 91/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.3224e-07\n",
      "Epoch 92/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.1639e-07\n",
      "Epoch 93/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0765e-07\n",
      "Epoch 94/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 9.1027e-08\n",
      "Epoch 95/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.7188e-08\n",
      "Epoch 96/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.2885e-08\n",
      "Epoch 97/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 7.0744e-08\n",
      "Epoch 98/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.8958e-08\n",
      "Epoch 99/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.0641e-08\n",
      "Epoch 100/100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.2128e-08\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8016 - loss: 2.1432\n",
      "Test loss: 2.016934394836426\n",
      "Test accuracy: 0.807624876499176\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_encoded = to_categorical(y_train - 1, num_classes=6)  # Subtract 1 here\n",
    "y_test_encoded = to_categorical(y_test - 1, num_classes=6)  # Subtract 1 here\n",
    "\n",
    "# Convert DataFrame to numpy array and then reshape\n",
    "X_train_reshaped = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    " \n",
    "# Add a convolutional layer \n",
    "model.add(Conv1D(32, 3, activation='relu', input_shape=(16335, 1)))  # Adjust the input shape here\n",
    "\n",
    "# Flatten the data for input to Dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add an output layer \n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train_encoded, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test_reshaped, y_test_encoded, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       665\n",
      "           1       0.84      0.91      0.87        96\n",
      "           2       0.82      0.88      0.85       575\n",
      "           3       0.74      0.69      0.71       289\n",
      "           4       0.32      0.16      0.21        45\n",
      "           5       0.79      0.80      0.80       612\n",
      "\n",
      "    accuracy                           0.81      2282\n",
      "   macro avg       0.73      0.71      0.71      2282\n",
      "weighted avg       0.80      0.81      0.80      2282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the classes\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test - 1, y_pred_classes))  # Subtract 1 here if your classes start from 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
